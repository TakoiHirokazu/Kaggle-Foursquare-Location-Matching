{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:30.440496Z",
     "iopub.status.busy": "2022-05-16T02:05:30.439885Z",
     "iopub.status.idle": "2022-05-16T02:05:37.372907Z",
     "shell.execute_reply": "2022-05-16T02:05:37.372111Z",
     "shell.execute_reply.started": "2022-05-16T02:05:30.440453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-02 11:49:00.595268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Library\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import DistilBertModel, DistilBertTokenizer,AutoModel,AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import cudf, cuml, cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_ema import ExponentialMovingAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.375136Z",
     "iopub.status.busy": "2022-05-16T02:05:37.374755Z",
     "iopub.status.idle": "2022-05-16T02:05:37.380182Z",
     "shell.execute_reply": "2022-05-16T02:05:37.379140Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.375095Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Constant\n",
    "# =========================\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TARGET = \"point_of_interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.381872Z",
     "iopub.status.busy": "2022-05-16T02:05:37.381599Z",
     "iopub.status.idle": "2022-05-16T02:05:37.444538Z",
     "shell.execute_reply": "2022-05-16T02:05:37.443795Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.381837Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Settings\n",
    "# =========================\n",
    "exp = \"115\"\n",
    "if not os.path.exists(f\"../output/exp/ex{exp}\"):\n",
    "    os.makedirs(f\"../output/exp/ex{exp}\")\n",
    "    os.makedirs(f\"../output/exp/ex{exp}/model\")\n",
    "LOGGER_PATH = f\"../output/exp/ex{exp}/ex_{exp}.txt\"\n",
    "MODEL_PATH_SAVE = f\"../output/exp/ex{exp}/ex{exp}\"\n",
    "MODEL_PATH_BASE = f\"../output/exp/ex{exp}/model/ex{exp}\"\n",
    "MODEL_PATH = \"microsoft/mdeberta-v3-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "val_fold = 0 \n",
    "# config\n",
    "debug = False\n",
    "SEED = 0\n",
    "BATCH_SIZE = 64\n",
    "if debug:\n",
    "    BATCH_SIZE = 16\n",
    "iters_to_accumulate = 1\n",
    "n_epochs = 6\n",
    "max_len = 128\n",
    "weight_decay = 0.1\n",
    "beta = (0.9, 0.98)\n",
    "scheduler_type = \"cosine\"\n",
    "lr = 1e-5\n",
    "head_type = \"linear\"\n",
    "lr_head = 1e-4\n",
    "num_warmup_steps_rate = 0.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "clip_grad_norm = 1\n",
    "fgm_start_epoch = 2\n",
    "ema_decay = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.447731Z",
     "iopub.status.busy": "2022-05-16T02:05:37.447033Z",
     "iopub.status.idle": "2022-05-16T02:05:37.460693Z",
     "shell.execute_reply": "2022-05-16T02:05:37.459912Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.447684Z"
    }
   },
   "outputs": [],
   "source": [
    "class FourSquareDataset(Dataset):\n",
    "    def __init__(self, text, near_text,num_features, tokenizer, max_len,labels=None):\n",
    "        self.text = text\n",
    "        self.near_text = near_text\n",
    "        self.num_features = num_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        near_text = self.near_text[item]\n",
    "        inputs = self.tokenizer(\n",
    "            text,near_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        num_feature = self.num_features[item]\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[item]\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"num_feature\" : torch.tensor(num_feature, dtype=torch.float32),\n",
    "                \"label\" : torch.tensor(label, dtype=torch.float32),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"num_feature\" : torch.tensor(num_feature, dtype=torch.float32),\n",
    "            }\n",
    "    \n",
    "    \n",
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, in_features, max_length=max_len, num_layers=1, nhead=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer=nn.TransformerEncoderLayer(d_model=in_features, nhead=nhead),\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "        self.row_fc = nn.Linear(in_features, 1)\n",
    "        self.out_features = max_length\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.transformer(x)\n",
    "        out = self.row_fc(out).squeeze(-1)\n",
    "        p1d = (0, self.out_features - out.shape[-1])\n",
    "        out = F.pad(out, p1d, \"constant\", 0)\n",
    "        return out\n",
    "\n",
    "    \n",
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "            self.backup = {} \n",
    "        \n",
    "    \n",
    "class bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bert_model, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "        self.head_type = head_type\n",
    "        encoder_feature_size = 768\n",
    "        if self.head_type == \"transformer\":\n",
    "            self.transformer_head = TransformerHead(\n",
    "                    in_features=768,\n",
    "                    max_length=max_len,\n",
    "                    num_layers=1,\n",
    "                    nhead=8,\n",
    "                )\n",
    "            encoder_feature_size = self.transformer_head.out_features\n",
    "        self.ln1 = nn.LayerNorm(encoder_feature_size)\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(encoder_feature_size,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(90,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(128 + 128,64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64,1),\n",
    "           )\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids,num_features):\n",
    "        if self.head_type == \"transformer\":\n",
    "            out = self.model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state']\n",
    "            out = self.transformer_head(out)\n",
    "        else:\n",
    "            out = self.model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n",
    "        out =  self.ln1(out)\n",
    "        out = self.linear1(out)\n",
    "        out2 = self.linear2(num_features)\n",
    "        out = torch.cat([out,out2],axis=-1)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def collate(d,train=True):\n",
    "    mask_len = int(d[\"attention_mask\"].sum(axis=1).max())\n",
    "    if train:\n",
    "        return {\"input_ids\" : d['input_ids'][:,:mask_len],\n",
    "                \"attention_mask\" : d['attention_mask'][:,:mask_len],\n",
    "                \"token_type_ids\" : d[\"token_type_ids\"][:,:mask_len],\n",
    "                 \"label\" : d['label'],\n",
    "                 \"num_feature\" : d[\"num_feature\"]}\n",
    "    else:\n",
    "        return {\"input_ids\" : d['input_ids'][:,:mask_len],\n",
    "                \"attention_mask\" : d['attention_mask'][:,:mask_len],\n",
    "                \"token_type_ids\" : d[\"token_type_ids\"][:,:mask_len],\n",
    "                 \"num_feature\" : d[\"num_feature\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def join(df):\n",
    "        x = [str(e) for e in list(df)]\n",
    "        return \" \".join(x)\n",
    "    \n",
    "def get_comp_score(val,pred):\n",
    "    train_raw = pd.read_csv(TRAIN_PATH)\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_raw, train_raw[TARGET],\n",
    "                                                    train_raw[TARGET])):\n",
    "        train_raw.loc[val_idx, \"set\"] = i\n",
    "    val_ = val.copy()\n",
    "    val_[\"pred\"] = pred\n",
    "    val_tr_ = val_[val_[\"pred\"] >= 0.5].reset_index(drop=True)\n",
    "    #del val_tr\n",
    "    gc.collect()\n",
    "    val_id = train_raw[train_raw[\"set\"] == val_fold][\"id\"].unique()\n",
    "    #del val_\n",
    "    gc.collect()\n",
    "    val_id_match = pd.DataFrame()\n",
    "    val_id_match[\"id\"] = val_id\n",
    "    val_id_match[\"near_id\"] = val_id\n",
    "    val_all = pd.concat([val_id_match,val_tr_[[\"id\",\"near_id\"]]]).reset_index(drop=True)\n",
    "    #val_all = val_all[[\"id\",\"near_id\"]].reset_index(drop=True)\n",
    "    val_all_ = val_all.copy()\n",
    "    val_all_.columns = [\"near_id\",\"id\"]\n",
    "    val_all = pd.concat([val_all,val_all_]).reset_index(drop=True)\n",
    "    val_all = val_all.drop_duplicates(subset=[\"id\",\"near_id\"]).reset_index(drop=True)\n",
    "    del val_all_\n",
    "    gc.collect()\n",
    "    val_all = val_all.merge(train_raw[[\"id\",\"point_of_interest\"]],how=\"left\",on=\"id\").reset_index(drop=True)\n",
    "    id2poi = get_id2poi(val_all)\n",
    "    poi2ids = get_poi2ids(val_all)\n",
    "    docs = val_all.groupby(\"id\")[\"near_id\"].apply(join)\n",
    "    docs = docs.reset_index()\n",
    "    docs.columns = [\"id\",\"matches\"]\n",
    "    scores = []\n",
    "    for id_str, matches in zip(docs['id'].to_numpy(), docs['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-02 11:49:07,000 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.462347Z",
     "iopub.status.busy": "2022-05-16T02:05:37.462014Z",
     "iopub.status.idle": "2022-05-16T02:05:44.599422Z",
     "shell.execute_reply": "2022-05-16T02:05:44.598528Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.462307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3170: DtypeWarning: Columns (17,20,25,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Main\n",
    "# ============================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train_fold0 = pd.read_csv(\"../output/exp/ex062/ex062_fe.csv\")\n",
    "train_fold1 = pd.read_csv(\"../output/exp/ex063/ex063_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_fold0 = train_fold0[:1000]\n",
    "    train_fold1 = train_fold1[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================================================\n",
    "# Spatial location features\n",
    "# ===============================================================================\n",
    "\n",
    "# ===============================================================================\n",
    "# Get manhattan distance\n",
    "# ===============================================================================\n",
    "def manhattan(lat1, long1, lat2, long2):\n",
    "    return np.abs(lat2 - lat1) + np.abs(long2 - long1)\n",
    "\n",
    "# ===============================================================================\n",
    "# Get haversine distance\n",
    "# ===============================================================================\n",
    "def vectorized_haversine(lats1, lats2, longs1, longs2):\n",
    "    # radius = 6371\n",
    "    radius = 1\n",
    "    dlat=np.radians(lats2 - lats1)\n",
    "    dlon=np.radians(longs2 - longs1)\n",
    "    a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lats1)) \\\n",
    "        * np.cos(np.radians(lats2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    d = radius * c\n",
    "    return d\n",
    "\n",
    "# ===============================================================================\n",
    "# Compute distances + Euclidean\n",
    "# ===============================================================================\n",
    "def add_lat_lon_distance_features(df):\n",
    "    lat1 = df['latitude']\n",
    "    lat2 = df['near_latitude']\n",
    "    lon1 = df['longitude']\n",
    "    lon2 = df['near_longitude']\n",
    "    df['latdiff'] = (lat1 - lat2)\n",
    "    df['londiff'] = (lon1 - lon2)\n",
    "    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n",
    "    df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n",
    "    df['haversine'] = vectorized_haversine(lat1, lat2, lon1, lon2)\n",
    "    df[\"x\"] = np.cos(np.radians(df[\"latitude\"]))*np.cos(np.radians(df[\"longitude\"]))\n",
    "    df[\"y\"] = np.sin(np.radians(df[\"latitude\"]))*np.cos(np.radians(df[\"longitude\"]))\n",
    "    df[\"z\"] = np.sin(np.radians(df[\"longitude\"]))\n",
    "    df[\"near_x\"] = np.cos(np.radians(df[\"near_latitude\"]))*np.cos(np.radians(df[\"near_longitude\"]))\n",
    "    df[\"near_y\"] = np.sin(np.radians(df[\"near_latitude\"]))*np.cos(np.radians(df[\"near_longitude\"]))\n",
    "    df[\"near_z\"] = np.sin(np.radians(df[\"near_longitude\"]))\n",
    "    df[\"dot\"] = df[\"x\"]*df[\"near_x\"]+df[\"y\"]*df[\"near_y\"]+df[\"z\"]*df[\"near_z\"]\n",
    "\n",
    "\n",
    "    col_64 = list(df.dtypes[df.dtypes == np.float64].index)\n",
    "    for col in col_64:\n",
    "        df[col] = df[col].astype(np.float32)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-02 11:50:53,615 - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-07-02 11:50:53,616 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "train_fold0 = add_lat_lon_distance_features(train_fold0)\n",
    "train_fold1 = add_lat_lon_distance_features(train_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textの作成\n",
    "train_fold0[\"text\"] = train_fold0['name'].astype(str).str.lower() + \" \" + train_fold0['categories'].astype(str).str.lower()+\\\n",
    "                      train_fold0['address'].astype(str).str.lower() + \" \" + train_fold0['city'].astype(str).str.lower() + \" \" + train_fold0['state'].astype(str).str.lower() \n",
    "train_fold0[\"near_text\"] = train_fold0['near_name'].astype(str).str.lower() + \" \" + train_fold0['near_categories'].astype(str).str.lower()+\\\n",
    "                      train_fold0['near_address'].astype(str).str.lower() + \" \" + train_fold0['near_city'].astype(str).str.lower() + \" \" + train_fold0['near_state'].astype(str).str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textの作成\n",
    "train_fold1[\"text\"] = train_fold1['name'].astype(str).str.lower() + \" \" + train_fold1['categories'].astype(str).str.lower()+\\\n",
    "                      train_fold1['address'].astype(str).str.lower() + \" \" + train_fold1['city'].astype(str).str.lower() + \" \" + train_fold1['state'].astype(str).str.lower() \n",
    "train_fold1[\"near_text\"] = train_fold1['near_name'].astype(str).str.lower() + \" \" + train_fold1['near_categories'].astype(str).str.lower()+\\\n",
    "                      train_fold1['near_address'].astype(str).str.lower() + \" \" + train_fold1['near_city'].astype(str).str.lower() + \" \" + train_fold1['near_state'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"latitude\", \"longitude\", \"near_latitude\", \"near_longitude\",\n",
    "           \"latdiff\", \"londiff\", \"manhattan\", \"euclidean\", \"haversine\",\n",
    "           \"x\", \"y\", \"z\", \"near_x\", \"near_y\", \"near_z\", \"dot\",\n",
    "           'name_gesh', 'name_leven', 'name_jaro',\n",
    "           'address_gesh', 'address_leven', 'address_jaro', 'city_gesh',\n",
    "           'city_leven', 'city_jaro', 'state_gesh', 'state_leven', 'state_jaro',\n",
    "           'zip_gesh', 'zip_leven', 'zip_jaro', 'url_gesh', 'url_leven',\n",
    "           'url_jaro', 'phone_gesh', 'phone_leven','phone_jaro', 'categories_gesh', 'categories_leven', 'categories_jaro',\n",
    "           'distance', 'distance_rank', 'name_gesh_mean', 'name_gesh_max',\n",
    "           'near_name_gesh_mean', 'near_name_gesh_max', 'name_gesh_mean_rate',\n",
    "           'name_gesh_max_rate', 'near_name_gesh_mean_rate',\n",
    "           'near_name_gesh_max_rate', 'name_leven_mean', 'name_leven_min',\n",
    "           'near_name_leven_mean', 'near_name_leven_min', 'name_leven_mean_rate',\n",
    "           'name_leven_min_rate', 'near_name_leven_mean_rate',\n",
    "           'near_name_leven_min_rate', 'name_jaro_mean', 'name_jaro_max',\n",
    "           'near_name_jaro_mean', 'near_name_jaro_max', 'name_jaro_mean_rate',\n",
    "           'name_jaro_max_rate', 'near_name_jaro_mean_rate',\n",
    "           'near_name_jaro_max_rate', 'categories_gesh_mean',\n",
    "           'categories_gesh_max', 'near_categories_gesh_mean',\n",
    "           'near_categories_gesh_max', 'categories_gesh_mean_rate',\n",
    "           'categories_gesh_max_rate', 'near_categories_gesh_mean_rate',\n",
    "           'near_categories_gesh_max_rate', 'categories_leven_mean',\n",
    "           'categories_leven_min', 'near_categories_leven_mean',\n",
    "           'near_categories_leven_min', 'categories_leven_mean_rate',\n",
    "           'categories_leven_min_rate', 'near_categories_leven_mean_rate',\n",
    "           'near_categories_leven_min_rate', 'categories_jaro_mean',\n",
    "           'categories_jaro_max', 'near_categories_jaro_mean',\n",
    "           'near_categories_jaro_max','categories_jaro_mean_rate', 'categories_jaro_max_rate',\n",
    "           'near_categories_jaro_mean_rate', 'near_categories_jaro_max_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_fold0 = train_fold0[num_cols].values.astype(np.float32)\n",
    "num_features_fold1 = train_fold1[num_cols].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:04, 18.06it/s]\n"
     ]
    }
   ],
   "source": [
    "num_features_concat = np.concatenate([num_features_fold0,num_features_fold1],axis=0)\n",
    "mean_std_dict = {}\n",
    "cols2num_dict = {}\n",
    "for n,c in tqdm(enumerate(num_cols)):\n",
    "    if c in [\"latitude\",\"longitude\"]:\n",
    "        mean = train[c].mean()\n",
    "        std = train[c].std()\n",
    "        mean_std_dict[c] = [mean,std]\n",
    "    elif c in [\"x\", \"y\", \"z\", \"near_x\", \"near_y\", \"near_z\", \"dot\"]:\n",
    "        mean = 0\n",
    "        std = 1\n",
    "        mean_std_dict[c] = [mean,std]\n",
    "    else:\n",
    "        num_features_concat[num_features_concat[:,n] == np.inf,n] = np.nan\n",
    "        num_features_concat[num_features_concat[:,n] == -np.inf,n] = np.nan\n",
    "        mean = np.nanmean(num_features_concat[:,n])\n",
    "        std = np.nanstd(num_features_concat[:,n])\n",
    "        mean_std_dict[c] = [mean,std]\n",
    "    cols2num_dict[c] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': [26.87459868745177, 23.144740576788625],\n",
       " 'longitude': [20.70497351331466, 82.6778436146614],\n",
       " 'near_latitude': [22.377329, 23.80125],\n",
       " 'near_longitude': [47.604324, 72.81156],\n",
       " 'latdiff': [0.0026245795, 0.60088676],\n",
       " 'londiff': [-0.004257245, 1.7946571],\n",
       " 'manhattan': [0.21769507, 2.1573222],\n",
       " 'euclidean': [0.17776342, 1.8842192],\n",
       " 'haversine': [0.002634386, 0.023196151],\n",
       " 'x': [0, 1],\n",
       " 'y': [0, 1],\n",
       " 'z': [0, 1],\n",
       " 'near_x': [0, 1],\n",
       " 'near_y': [0, 1],\n",
       " 'near_z': [0, 1],\n",
       " 'dot': [0, 1],\n",
       " 'name_gesh': [0.535247, 0.28312334],\n",
       " 'name_leven': [12.289453, 8.717725],\n",
       " 'name_jaro': [0.6814999, 0.278118],\n",
       " 'address_gesh': [0.55750847, 0.32904968],\n",
       " 'address_leven': [11.519652, 11.330601],\n",
       " 'address_jaro': [0.68748957, 0.28175715],\n",
       " 'city_gesh': [0.78845024, 0.33762273],\n",
       " 'city_leven': [2.639476, 4.342476],\n",
       " 'city_jaro': [0.8420279, 0.29122102],\n",
       " 'state_gesh': [0.7989922, 0.3356451],\n",
       " 'state_leven': [2.618497, 4.6914506],\n",
       " 'state_jaro': [0.8407704, 0.2916656],\n",
       " 'zip_gesh': [0.90403444, 0.18740492],\n",
       " 'zip_leven': [0.6094352, 1.2436553],\n",
       " 'zip_jaro': [0.9485774, 0.11972204],\n",
       " 'url_gesh': [0.8163289, 0.22502218],\n",
       " 'url_leven': [15.599948, 23.641768],\n",
       " 'url_jaro': [0.9568382, 0.07457281],\n",
       " 'phone_gesh': [0.7717348, 0.24688902],\n",
       " 'phone_leven': [3.4015062, 3.4060066],\n",
       " 'phone_jaro': [0.85937643, 0.16305733],\n",
       " 'categories_gesh': [0.5930225, 0.32405168],\n",
       " 'categories_leven': [10.481613, 10.402995],\n",
       " 'categories_jaro': [0.72280174, 0.2595957],\n",
       " 'distance': [3.5818813, 170.23347],\n",
       " 'distance_rank': [11.811793, 10.082427],\n",
       " 'name_gesh_mean': [0.40428534, 0.14291741],\n",
       " 'name_gesh_max': [0.8063714, 0.17690912],\n",
       " 'near_name_gesh_mean': [0.40961915, 0.14795418],\n",
       " 'near_name_gesh_max': [0.8105544, 0.17704241],\n",
       " 'name_gesh_mean_rate': [1.3989464, 1.0484291],\n",
       " 'name_gesh_max_rate': [0.6530004, 0.29792878],\n",
       " 'near_name_gesh_mean_rate': [1.3906603, 1.0571884],\n",
       " 'near_name_gesh_max_rate': [0.65022796, 0.298543],\n",
       " 'name_leven_mean': [14.080113, 5.990362],\n",
       " 'name_leven_min': [5.0102377, 5.371386],\n",
       " 'near_name_leven_mean': [13.955088, 6.017419],\n",
       " 'near_name_leven_min': [4.898045, 5.3486223],\n",
       " 'name_leven_mean_rate': [0.8712333, 0.6081466],\n",
       " 'name_leven_min_rate': [3.0417712, 3.5432434],\n",
       " 'near_name_leven_mean_rate': [0.88712335, 0.6753299],\n",
       " 'near_name_leven_min_rate': [3.103235, 3.6311984],\n",
       " 'name_jaro_mean': [0.59393793, 0.14204046],\n",
       " 'name_jaro_max': [0.922182, 0.113545366],\n",
       " 'near_name_jaro_mean': [0.5990131, 0.14566767],\n",
       " 'near_name_jaro_max': [0.9246227, 0.112311274],\n",
       " 'name_jaro_mean_rate': [1.17415, 0.75928885],\n",
       " 'name_jaro_max_rate': [0.73229, 0.27749673],\n",
       " 'near_name_jaro_mean_rate': [1.1688758, 0.76395464],\n",
       " 'near_name_jaro_max_rate': [0.73066276, 0.2782531],\n",
       " 'categories_gesh_mean': [0.45021054, 0.16041645],\n",
       " 'categories_gesh_max': [0.8934652, 0.18464169],\n",
       " 'near_categories_gesh_mean': [0.45295003, 0.1621628],\n",
       " 'near_categories_gesh_max': [0.89459765, 0.18372972],\n",
       " 'categories_gesh_mean_rate': [1.3444637, 0.71897376],\n",
       " 'categories_gesh_max_rate': [0.65741974, 0.31307998],\n",
       " 'near_categories_gesh_mean_rate': [1.3444862, 0.7269207],\n",
       " 'near_categories_gesh_max_rate': [0.65774196, 0.31624898],\n",
       " 'categories_leven_mean': [13.444308, 6.6503487],\n",
       " 'categories_leven_min': [2.9128094, 5.704073],\n",
       " 'near_categories_leven_mean': [13.410719, 6.695578],\n",
       " 'near_categories_leven_min': [2.9045722, 5.7120137],\n",
       " 'categories_leven_mean_rate': [0.7467077, 0.7704559],\n",
       " 'categories_leven_min_rate': [2.024397, 1.5368462],\n",
       " 'near_categories_leven_mean_rate': [0.75811625, 0.9181195],\n",
       " 'near_categories_leven_min_rate': [2.0135462, 1.5323894],\n",
       " 'categories_jaro_mean': [0.6192549, 0.1293145],\n",
       " 'categories_jaro_max': [0.94835114, 0.116735004],\n",
       " 'near_categories_jaro_mean': [0.6217563, 0.13072947],\n",
       " 'near_categories_jaro_max': [0.94931644, 0.11579985],\n",
       " 'categories_jaro_mean_rate': [1.1727746, 0.40448013],\n",
       " 'categories_jaro_max_rate': [0.7595728, 0.24773462],\n",
       " 'near_categories_jaro_mean_rate': [1.1711832, 0.4081076],\n",
       " 'near_categories_jaro_max_rate': [0.75936586, 0.24919231]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 96.93it/s] \n"
     ]
    }
   ],
   "source": [
    "for n,c in tqdm(enumerate(num_cols)):\n",
    "    num_features_fold0[num_features_fold0[:,n] == np.inf,n] = np.nan\n",
    "    num_features_fold0[num_features_fold0[:,n] == -np.inf,n] = np.nan\n",
    "    num_features_fold0[:,n] = (num_features_fold0[:,n] - mean_std_dict[c][0]) / (mean_std_dict[c][1]) \n",
    "    \n",
    "    num_features_fold1[num_features_fold1[:,n] == np.inf,n] = np.nan\n",
    "    num_features_fold1[num_features_fold1[:,n] == -np.inf,n] = np.nan\n",
    "    num_features_fold1[:,n] = (num_features_fold1[:,n] - mean_std_dict[c][0]) / (mean_std_dict[c][1]) \n",
    "num_features_fold0 = np.nan_to_num(num_features_fold0)\n",
    "num_features_fold1 = np.nan_to_num(num_features_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train_fold0,train_fold1]).reset_index(drop=True)\n",
    "num_features_all = np.concatenate([num_features_fold0,num_features_fold1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:0==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "100%|██████████| 66309/66309 [2:49:38<00:00,  6.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:1==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 66309/66309 [2:49:52<00:00,  6.51it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:2==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 66309/66309 [5:08:24<00:00,  3.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:3==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 66309/66309 [5:06:31<00:00,  3.61it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:4==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66309/66309 [5:07:16<00:00,  3.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:5==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 47542/66309 [3:39:03<1:28:38,  3.53it/s]"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# train\n",
    "# ================================\n",
    "with timer(\"mdeberta_base\"):\n",
    "    set_seed(SEED)\n",
    "    train_text,train_near_text, train_num_features, train_labels = \\\n",
    "    train_all[\"text\"].values, train_all[\"near_text\"].values,num_features_all, train_all[\"target\"].values\n",
    "    train_ = FourSquareDataset(train_text,train_near_text,train_num_features,\n",
    "                               tokenizer,max_len,train_labels)\n",
    "        \n",
    "    # loader\n",
    "    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True ,pin_memory=True,num_workers=8)\n",
    "        \n",
    "    # model\n",
    "    model = bert_model()\n",
    "    model = model.to(device)\n",
    "    model2 = bert_model()\n",
    "    model2 = model2.to(device)\n",
    "    ema = ExponentialMovingAverage(model.parameters(), decay=ema_decay)\n",
    "    fgm = FGM(model)\n",
    "\n",
    "    # optimizer, scheduler\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay, \"lr\": lr},\n",
    "        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0, \"lr\": lr},\n",
    "        {'params': [p for n, p in model.named_parameters() if \"model\" not in n], 'weight_decay': weight_decay, \"lr\": lr_head}, # head params\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=lr,\n",
    "                      betas=beta,\n",
    "                      weight_decay=weight_decay,\n",
    "                      )\n",
    "    num_train_optimization_steps = int(len(train_loader) * n_epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * num_warmup_steps_rate)\n",
    "    if scheduler_type == \"cosine\":\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer,\n",
    "                    num_warmup_steps=num_warmup_steps,\n",
    "                    num_training_steps=num_train_optimization_steps,\n",
    "                    num_cycles=0.5,\n",
    "                )\n",
    "    elif scheduler_type == \"linear\":\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                    num_warmup_steps=num_warmup_steps,\n",
    "                                                    num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"============start epoch:{epoch}==============\")\n",
    "        model.train() \n",
    "        train_losses_batch = []\n",
    "        for i, d in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "            d = collate(d)\n",
    "            ids = d[\"input_ids\"].to(device)\n",
    "            mask = d['attention_mask'].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "            labels = d['label'].to(device)\n",
    "            num_features = d[\"num_feature\"].to(device)\n",
    "            labels = labels.unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(ids,mask,token_type_ids,num_features)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "            if epoch >= fgm_start_epoch:\n",
    "                fgm.attack()\n",
    "                output_adv = model(ids,mask,token_type_ids,num_features)\n",
    "                loss_adv = criterion(output_adv, labels)\n",
    "                loss_adv.backward() \n",
    "                fgm.restore() \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            ema.update()\n",
    "        if epoch >= 3:\n",
    "            torch.save(model.state_dict(), MODEL_PATH_SAVE + f\"_{epoch}.pth\") # Saving current best model\n",
    "            ema.copy_to(model2.parameters())\n",
    "            torch.save(model2.state_dict(), MODEL_PATH_BASE + f\"_{epoch}_ema.pth\") # Saving current best ema model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
