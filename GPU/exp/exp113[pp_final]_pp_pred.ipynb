{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:30.440496Z",
     "iopub.status.busy": "2022-05-16T02:05:30.439885Z",
     "iopub.status.idle": "2022-05-16T02:05:37.372907Z",
     "shell.execute_reply": "2022-05-16T02:05:37.372111Z",
     "shell.execute_reply.started": "2022-05-16T02:05:30.440453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 20:48:29.726571: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Library\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import DistilBertModel, DistilBertTokenizer,AutoModel,AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import cudf, cuml, cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import gc\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.375136Z",
     "iopub.status.busy": "2022-05-16T02:05:37.374755Z",
     "iopub.status.idle": "2022-05-16T02:05:37.380182Z",
     "shell.execute_reply": "2022-05-16T02:05:37.379140Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.375095Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Constant\n",
    "# =========================\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TARGET = \"point_of_interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.381872Z",
     "iopub.status.busy": "2022-05-16T02:05:37.381599Z",
     "iopub.status.idle": "2022-05-16T02:05:37.444538Z",
     "shell.execute_reply": "2022-05-16T02:05:37.443795Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.381837Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Settings\n",
    "# =========================\n",
    "exp = \"113\"\n",
    "if not os.path.exists(f\"../output/exp/ex{exp}\"):\n",
    "    os.makedirs(f\"../output/exp/ex{exp}\")\n",
    "    os.makedirs(f\"../output/exp/ex{exp}/model\")\n",
    "LOGGER_PATH = f\"../output/exp/ex{exp}/ex_{exp}.txt\"\n",
    "MODEL_PATH_BASE = f\"../output/exp/ex{exp}/model/ex{exp}\"\n",
    "MODEL_PATH = \"microsoft/mdeberta-v3-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "val_fold = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def join(df):\n",
    "    x = [str(e) for e in list(df)]\n",
    "    return \" \".join(x)\n",
    "\n",
    "def id_sort(id_near_id):\n",
    "    id_near_id = \"-\".join(sorted(id_near_id.split(\"-\")))\n",
    "    return id_near_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 20:48:36,197 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.462347Z",
     "iopub.status.busy": "2022-05-16T02:05:37.462014Z",
     "iopub.status.idle": "2022-05-16T02:05:44.599422Z",
     "shell.execute_reply": "2022-05-16T02:05:44.598528Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.462307Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Main\n",
    "# ============================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train_fold0 = pd.read_csv(\"../output/exp/ex062/ex062_pred.csv\")\n",
    "train_fold0 = train_fold0.merge(train[[\"id\",\"country\"]],how=\"left\",on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = pd.read_csv(\"../output/exp/ex087/ex087_pred.csv\")\n",
    "cat.columns = [\"id\",\"near_id\",\"cat_pred\"]\n",
    "train_fold0 = train_fold0.merge(cat,how=\"left\",on=[\"id\",\"near_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deberta = np.load(f\"../output/exp/ex_charm_05/oof.npy\")\n",
    "roberta = np.load(f\"../output/exp/ex100/oof.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv(TRAIN_PATH)\n",
    "kf = GroupKFold(n_splits=2)\n",
    "for i, (trn_idx, val_idx) in enumerate(kf.split(train_raw, train_raw[TARGET],\n",
    "                                                train_raw[TARGET])):\n",
    "    train_raw.loc[val_idx, \"set\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.013225176172449034\n",
    "w2 = 0.28751060139700985\n",
    "w3 = 0.3813813593361608\n",
    "w4 = 0.31788286309438024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-07 20:49:44,620 - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-07-07 20:49:44,621 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "train_fold0[\"ensemble\"] = train_fold0[\"pred\"]*w1 + roberta.reshape(-1)*w2 + deberta.reshape(-1)*w3 + train_fold0[\"cat_pred\"]*w4\n",
    "val_tr_ = train_fold0[train_fold0[\"ensemble\"] >= 0.5].reset_index(drop=True)\n",
    "val_id = train_raw[train_raw[\"set\"] == 0][\"id\"].unique()\n",
    "val_id_match = pd.DataFrame()\n",
    "val_id_match[\"id\"] = val_id\n",
    "val_id_match[\"near_id\"] = val_id\n",
    "val_all = pd.concat([val_id_match,val_tr_[[\"id\",\"near_id\"]]]).reset_index(drop=True)\n",
    "val_all_ = val_all.copy()\n",
    "val_all_.columns = [\"near_id\",\"id\"]\n",
    "val_all = pd.concat([val_all,val_all_]).reset_index(drop=True)\n",
    "val_all = val_all.drop_duplicates(subset=[\"id\",\"near_id\"]).reset_index(drop=True)\n",
    "del val_all_\n",
    "gc.collect()\n",
    "val_all = val_all.merge(train_raw[[\"id\",\"point_of_interest\"]],how=\"left\",on=\"id\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new pair\n",
    "train_fold0_pp = pd.read_csv(\"../output/exp/ex113/ex113_pp_new_pair_fold0.csv\")\n",
    "pp_pred = np.load(f\"../output/exp/ex{exp}/oof.npy\")\n",
    "train_fold0_pp[\"pp_pred\"] = pp_pred.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold0_pp[\"id_near_id_sort\"] = train_fold0_pp[\"id_near_id\"].map(id_sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold0_pp = train_fold0_pp.drop_duplicates(subset = \"id_near_id_sort\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 0.9166754754547339\n",
      "0.02 0.9166534470005328\n",
      "0.03 0.9166095356061639\n"
     ]
    }
   ],
   "source": [
    "# Threshold adjustment\n",
    "for i in [0.01,0.02,0.03]:\n",
    "    train_fold0_pp_ = train_fold0_pp[train_fold0_pp[\"pp_pred\"] > i][[\"id\",\"near_id\"]]\n",
    "    train_fold0_pp__ = train_fold0_pp_.copy()\n",
    "    train_fold0_pp__.columns = [\"near_id\",\"id\"]\n",
    "    train_fold0_pp_ = pd.concat([train_fold0_pp_,train_fold0_pp__]).reset_index(drop=True)\n",
    "    train_fold0_pp_ = train_fold0_pp_.drop_duplicates(subset=[\"id\",\"near_id\"]).reset_index(drop=True)\n",
    "    train_fold0_pp_ = train_fold0_pp_.merge(train_raw[[\"id\",\"point_of_interest\"]],how=\"left\",on=\"id\")\n",
    "    val_all_ = pd.concat([val_all,train_fold0_pp_[[\"id\",\"near_id\",\"point_of_interest\"]]])\n",
    "    id2poi = get_id2poi(val_all_)\n",
    "    poi2ids = get_poi2ids(val_all_)\n",
    "    docs = val_all_.groupby(\"id\")[\"near_id\"].apply(join)\n",
    "    docs = docs.reset_index()\n",
    "    docs.columns = [\"id\",\"matches\"]\n",
    "    score = get_score(docs)\n",
    "    print(i,score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
