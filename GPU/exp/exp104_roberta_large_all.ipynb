{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:30.440496Z",
     "iopub.status.busy": "2022-05-16T02:05:30.439885Z",
     "iopub.status.idle": "2022-05-16T02:05:37.372907Z",
     "shell.execute_reply": "2022-05-16T02:05:37.372111Z",
     "shell.execute_reply.started": "2022-05-16T02:05:30.440453Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 00:14:38.018916: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Library\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from transformers import DistilBertModel, DistilBertTokenizer,AutoModel,AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import cudf, cuml, cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "import gc\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.375136Z",
     "iopub.status.busy": "2022-05-16T02:05:37.374755Z",
     "iopub.status.idle": "2022-05-16T02:05:37.380182Z",
     "shell.execute_reply": "2022-05-16T02:05:37.379140Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.375095Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Constant\n",
    "# =========================\n",
    "TRAIN_PATH = \"../data/train.csv\"\n",
    "TARGET = \"point_of_interest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.381872Z",
     "iopub.status.busy": "2022-05-16T02:05:37.381599Z",
     "iopub.status.idle": "2022-05-16T02:05:37.444538Z",
     "shell.execute_reply": "2022-05-16T02:05:37.443795Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.381837Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Settings\n",
    "# =========================\n",
    "exp = \"104\"\n",
    "if not os.path.exists(f\"../output/exp/ex{exp}\"):\n",
    "    os.makedirs(f\"../output/exp/ex{exp}\")\n",
    "    os.makedirs(f\"../output/exp/ex{exp}/model\")\n",
    "LOGGER_PATH = f\"../output/exp/ex{exp}/ex_{exp}.txt\"\n",
    "MODEL_PATH_BASE = f\"../output/exp/ex{exp}/model/ex{exp}\"\n",
    "MODEL_PATH = \"xlm-roberta-large\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "val_fold = 0 \n",
    "# config\n",
    "SEED = 0\n",
    "BATCH_SIZE = 64\n",
    "iters_to_accumulate = 1\n",
    "n_epochs = 5\n",
    "max_len = 128\n",
    "weight_decay = 0.1\n",
    "beta = (0.9, 0.98)\n",
    "lr = 1e-5\n",
    "num_warmup_steps_rate = 0.1\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "clip_grad_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "def setup_logger(out_file=None, stderr=True, stderr_level=logging.INFO, file_level=logging.DEBUG):\n",
    "    LOGGER.handlers = []\n",
    "    LOGGER.setLevel(min(stderr_level, file_level))\n",
    "\n",
    "    if stderr:\n",
    "        handler = logging.StreamHandler(sys.stderr)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(stderr_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    if out_file is not None:\n",
    "        handler = logging.FileHandler(out_file)\n",
    "        handler.setFormatter(FORMATTER)\n",
    "        handler.setLevel(file_level)\n",
    "        LOGGER.addHandler(handler)\n",
    "\n",
    "    LOGGER.info(\"logger set up\")\n",
    "    return LOGGER\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.447731Z",
     "iopub.status.busy": "2022-05-16T02:05:37.447033Z",
     "iopub.status.idle": "2022-05-16T02:05:37.460693Z",
     "shell.execute_reply": "2022-05-16T02:05:37.459912Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.447684Z"
    }
   },
   "outputs": [],
   "source": [
    "class FourSquareDataset(Dataset):\n",
    "    def __init__(self, text, near_text,num_features, tokenizer, max_len,labels=None):\n",
    "        self.text = text\n",
    "        self.near_text = near_text\n",
    "        self.num_features = num_features\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.text[item]\n",
    "        near_text = self.near_text[item]\n",
    "        inputs = self.tokenizer(\n",
    "            text,near_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        num_feature = self.num_features[item]\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[item]\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"num_feature\" : torch.tensor(num_feature, dtype=torch.float32),\n",
    "                \"label\" : torch.tensor(label, dtype=torch.float32),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"num_feature\" : torch.tensor(num_feature, dtype=torch.float32),\n",
    "            }\n",
    "    \n",
    "    \n",
    "class bert_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(bert_model, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "        self.ln1 = nn.LayerNorm(1024)\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(1024,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(76,128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2))\n",
    "        \n",
    "        self.linear3 = nn.Sequential(\n",
    "            nn.Linear(128 + 128,64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64,1),\n",
    "           )\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids,num_features):\n",
    "        # pooler\n",
    "        out = self.model(ids, attention_mask=mask,token_type_ids=token_type_ids)['last_hidden_state'][:,0,:]\n",
    "        out =  self.ln1(out)\n",
    "        out = self.linear1(out)\n",
    "        out2 = self.linear2(num_features)\n",
    "        out = torch.cat([out,out2],axis=-1)\n",
    "        out = self.linear3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "def collate(d,train=True):\n",
    "    mask_len = int(d[\"attention_mask\"].sum(axis=1).max())\n",
    "    if train:\n",
    "        return {\"input_ids\" : d['input_ids'][:,:mask_len],\n",
    "                \"attention_mask\" : d['attention_mask'][:,:mask_len],\n",
    "                \"token_type_ids\" : d[\"token_type_ids\"][:,:mask_len],\n",
    "                 \"label\" : d['label'],\n",
    "                 \"num_feature\" : d[\"num_feature\"]}\n",
    "    else:\n",
    "        return {\"input_ids\" : d['input_ids'][:,:mask_len],\n",
    "                \"attention_mask\" : d['attention_mask'][:,:mask_len],\n",
    "                \"token_type_ids\" : d[\"token_type_ids\"][:,:mask_len],\n",
    "                 \"num_feature\" : d[\"num_feature\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/columbia2131/foursquare-iou-metrics\n",
    "def get_id2poi(input_df: pd.DataFrame) -> dict:\n",
    "    return dict(zip(input_df['id'], input_df['point_of_interest']))\n",
    "\n",
    "def get_poi2ids(input_df: pd.DataFrame) -> dict:\n",
    "    return input_df.groupby('point_of_interest')['id'].apply(set).to_dict()\n",
    "\n",
    "def get_score(input_df: pd.DataFrame):\n",
    "    scores = []\n",
    "    for id_str, matches in zip(input_df['id'].to_numpy(), input_df['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()\n",
    "\n",
    "def join(df):\n",
    "        x = [str(e) for e in list(df)]\n",
    "        return \" \".join(x)\n",
    "    \n",
    "def get_comp_score(val,pred):\n",
    "    train_raw = pd.read_csv(TRAIN_PATH)\n",
    "    kf = GroupKFold(n_splits=2)\n",
    "    for i, (trn_idx, val_idx) in enumerate(kf.split(train_raw, train_raw[TARGET],\n",
    "                                                    train_raw[TARGET])):\n",
    "        train_raw.loc[val_idx, \"set\"] = i\n",
    "    val_ = val.copy()\n",
    "    val_[\"pred\"] = pred\n",
    "    val_tr_ = val_[val_[\"pred\"] >= 0.5].reset_index(drop=True)\n",
    "    #del val_tr\n",
    "    gc.collect()\n",
    "    val_id = train_raw[train_raw[\"set\"] == val_fold][\"id\"].unique()\n",
    "    #del val_\n",
    "    gc.collect()\n",
    "    val_id_match = pd.DataFrame()\n",
    "    val_id_match[\"id\"] = val_id\n",
    "    val_id_match[\"near_id\"] = val_id\n",
    "    val_all = pd.concat([val_id_match,val_tr_[[\"id\",\"near_id\"]]]).reset_index(drop=True)\n",
    "    #val_all = val_all[[\"id\",\"near_id\"]].reset_index(drop=True)\n",
    "    val_all_ = val_all.copy()\n",
    "    val_all_.columns = [\"near_id\",\"id\"]\n",
    "    val_all = pd.concat([val_all,val_all_]).reset_index(drop=True)\n",
    "    val_all = val_all.drop_duplicates(subset=[\"id\",\"near_id\"]).reset_index(drop=True)\n",
    "    del val_all_\n",
    "    gc.collect()\n",
    "    val_all = val_all.merge(train_raw[[\"id\",\"point_of_interest\"]],how=\"left\",on=\"id\").reset_index(drop=True)\n",
    "    id2poi = get_id2poi(val_all)\n",
    "    poi2ids = get_poi2ids(val_all)\n",
    "    docs = val_all.groupby(\"id\")[\"near_id\"].apply(join)\n",
    "    docs = docs.reset_index()\n",
    "    docs.columns = [\"id\",\"matches\"]\n",
    "    scores = []\n",
    "    for id_str, matches in zip(docs['id'].to_numpy(), docs['matches'].to_numpy()):\n",
    "        targets = poi2ids[id2poi[id_str]]\n",
    "        preds = set(matches.split())\n",
    "        score = len((targets & preds)) / len((targets | preds))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 00:14:50,052 - INFO - logger set up\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LOGGER = logging.getLogger()\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "setup_logger(out_file=LOGGER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-16T02:05:37.462347Z",
     "iopub.status.busy": "2022-05-16T02:05:37.462014Z",
     "iopub.status.idle": "2022-05-16T02:05:44.599422Z",
     "shell.execute_reply": "2022-05-16T02:05:44.598528Z",
     "shell.execute_reply.started": "2022-05-16T02:05:37.462307Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3170: DtypeWarning: Columns (17,20,25,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Main\n",
    "# ============================\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "train_fold0 = pd.read_csv(\"../output/share/ex062_063_fe/ex062_fe.csv\")\n",
    "train_fold1 = pd.read_csv(\"../output/share/ex062_063_fe/ex063_fe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in [\"name\",\"categories\",'latitude', 'longitude','address','city','state']:\n",
    "    if c not in train_fold0.columns:\n",
    "        print(c)\n",
    "    if \"near_\" + c not in train_fold0.columns:\n",
    "        print(\"near_\" + c )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-27 00:16:54,756 - INFO - Note: NumExpr detected 12 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2022-06-27 00:16:54,757 - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "# textの作成\n",
    "train_fold0[\"text\"] = train_fold0['name'].astype(str).str.lower() + \" \" + train_fold0['categories'].astype(str).str.lower()+\\\n",
    "                      train_fold0['address'].astype(str).str.lower() + \" \" + train_fold0['city'].astype(str).str.lower() + \" \" + train_fold0['state'].astype(str).str.lower() \n",
    "train_fold0[\"near_text\"] = train_fold0['near_name'].astype(str).str.lower() + \" \" + train_fold0['near_categories'].astype(str).str.lower()+\\\n",
    "                      train_fold0['near_address'].astype(str).str.lower() + \" \" + train_fold0['near_city'].astype(str).str.lower() + \" \" + train_fold0['near_state'].astype(str).str.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textの作成\n",
    "train_fold1[\"text\"] = train_fold1['name'].astype(str).str.lower() + \" \" + train_fold1['categories'].astype(str).str.lower()+\\\n",
    "                      train_fold1['address'].astype(str).str.lower() + \" \" + train_fold1['city'].astype(str).str.lower() + \" \" + train_fold1['state'].astype(str).str.lower() \n",
    "train_fold1[\"near_text\"] = train_fold1['near_name'].astype(str).str.lower() + \" \" + train_fold1['near_categories'].astype(str).str.lower()+\\\n",
    "                      train_fold1['near_address'].astype(str).str.lower() + \" \" + train_fold1['near_city'].astype(str).str.lower() + \" \" + train_fold1['near_state'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"latitude\",\"longitude\",'name_gesh', 'name_leven', 'name_jaro',\n",
    "       'address_gesh', 'address_leven', 'address_jaro', 'city_gesh',\n",
    "       'city_leven', 'city_jaro', 'state_gesh', 'state_leven', 'state_jaro',\n",
    "       'zip_gesh', 'zip_leven', 'zip_jaro', 'url_gesh', 'url_leven',\n",
    "       'url_jaro', 'phone_gesh', 'phone_leven','phone_jaro', 'categories_gesh', 'categories_leven', 'categories_jaro',\n",
    "       'distance', 'distance_rank', 'name_gesh_mean', 'name_gesh_max',\n",
    "       'near_name_gesh_mean', 'near_name_gesh_max', 'name_gesh_mean_rate',\n",
    "       'name_gesh_max_rate', 'near_name_gesh_mean_rate',\n",
    "       'near_name_gesh_max_rate', 'name_leven_mean', 'name_leven_min',\n",
    "       'near_name_leven_mean', 'near_name_leven_min', 'name_leven_mean_rate',\n",
    "       'name_leven_min_rate', 'near_name_leven_mean_rate',\n",
    "       'near_name_leven_min_rate', 'name_jaro_mean', 'name_jaro_max',\n",
    "       'near_name_jaro_mean', 'near_name_jaro_max', 'name_jaro_mean_rate',\n",
    "       'name_jaro_max_rate', 'near_name_jaro_mean_rate',\n",
    "       'near_name_jaro_max_rate', 'categories_gesh_mean',\n",
    "       'categories_gesh_max', 'near_categories_gesh_mean',\n",
    "       'near_categories_gesh_max', 'categories_gesh_mean_rate',\n",
    "       'categories_gesh_max_rate', 'near_categories_gesh_mean_rate',\n",
    "       'near_categories_gesh_max_rate', 'categories_leven_mean',\n",
    "       'categories_leven_min', 'near_categories_leven_mean',\n",
    "       'near_categories_leven_min', 'categories_leven_mean_rate',\n",
    "       'categories_leven_min_rate', 'near_categories_leven_mean_rate',\n",
    "       'near_categories_leven_min_rate', 'categories_jaro_mean',\n",
    "       'categories_jaro_max', 'near_categories_jaro_mean',\n",
    "       'near_categories_jaro_max','categories_jaro_mean_rate', 'categories_jaro_max_rate',\n",
    "       'near_categories_jaro_mean_rate', 'near_categories_jaro_max_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_fold0 = train_fold0[num_cols].values.astype(np.float32)\n",
    "num_features_fold1 = train_fold1[num_cols].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:04, 16.56it/s]\n"
     ]
    }
   ],
   "source": [
    "num_features_concat = np.concatenate([num_features_fold0,num_features_fold1],axis=0)\n",
    "mean_std_dict = {}\n",
    "cols2num_dict = {}\n",
    "for n,c in tqdm(enumerate(num_cols)):\n",
    "    if c in [\"latitude\",\"longitude\"]:\n",
    "        mean = train[c].mean()\n",
    "        std = train[c].std()\n",
    "        mean_std_dict[c] = [mean,std]\n",
    "    else:\n",
    "        num_features_concat[num_features_concat[:,n] == np.inf,n] = np.nan\n",
    "        num_features_concat[num_features_concat[:,n] == -np.inf,n] = np.nan\n",
    "        mean = np.nanmean(num_features_concat[:,n])\n",
    "        std = np.nanstd(num_features_concat[:,n])\n",
    "        mean_std_dict[c] = [mean,std]\n",
    "    cols2num_dict[c] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude': [26.87459868745177, 23.144740576788625],\n",
       " 'longitude': [20.70497351331466, 82.6778436146614],\n",
       " 'name_gesh': [0.535247, 0.28312334],\n",
       " 'name_leven': [12.289453, 8.717725],\n",
       " 'name_jaro': [0.6814999, 0.278118],\n",
       " 'address_gesh': [0.55750847, 0.32904968],\n",
       " 'address_leven': [11.519652, 11.330601],\n",
       " 'address_jaro': [0.68748957, 0.28175715],\n",
       " 'city_gesh': [0.78845024, 0.33762273],\n",
       " 'city_leven': [2.639476, 4.342476],\n",
       " 'city_jaro': [0.8420279, 0.29122102],\n",
       " 'state_gesh': [0.7989922, 0.3356451],\n",
       " 'state_leven': [2.618497, 4.6914506],\n",
       " 'state_jaro': [0.8407704, 0.2916656],\n",
       " 'zip_gesh': [0.90403444, 0.18740492],\n",
       " 'zip_leven': [0.6094352, 1.2436553],\n",
       " 'zip_jaro': [0.9485774, 0.11972204],\n",
       " 'url_gesh': [0.8163289, 0.22502218],\n",
       " 'url_leven': [15.599948, 23.641768],\n",
       " 'url_jaro': [0.9568382, 0.07457281],\n",
       " 'phone_gesh': [0.7717348, 0.24688902],\n",
       " 'phone_leven': [3.4015062, 3.4060066],\n",
       " 'phone_jaro': [0.85937643, 0.16305733],\n",
       " 'categories_gesh': [0.5930225, 0.32405168],\n",
       " 'categories_leven': [10.481613, 10.402995],\n",
       " 'categories_jaro': [0.72280174, 0.2595957],\n",
       " 'distance': [3.5818813, 170.23347],\n",
       " 'distance_rank': [11.811793, 10.082427],\n",
       " 'name_gesh_mean': [0.40428534, 0.14291741],\n",
       " 'name_gesh_max': [0.8063714, 0.17690912],\n",
       " 'near_name_gesh_mean': [0.40961915, 0.14795418],\n",
       " 'near_name_gesh_max': [0.8105544, 0.17704241],\n",
       " 'name_gesh_mean_rate': [1.3989464, 1.0484291],\n",
       " 'name_gesh_max_rate': [0.6530004, 0.29792878],\n",
       " 'near_name_gesh_mean_rate': [1.3906603, 1.0571884],\n",
       " 'near_name_gesh_max_rate': [0.65022796, 0.298543],\n",
       " 'name_leven_mean': [14.080113, 5.990362],\n",
       " 'name_leven_min': [5.0102377, 5.371386],\n",
       " 'near_name_leven_mean': [13.955088, 6.017419],\n",
       " 'near_name_leven_min': [4.898045, 5.3486223],\n",
       " 'name_leven_mean_rate': [0.8712333, 0.6081466],\n",
       " 'name_leven_min_rate': [3.0417712, 3.5432434],\n",
       " 'near_name_leven_mean_rate': [0.88712335, 0.6753299],\n",
       " 'near_name_leven_min_rate': [3.103235, 3.6311984],\n",
       " 'name_jaro_mean': [0.59393793, 0.14204046],\n",
       " 'name_jaro_max': [0.922182, 0.113545366],\n",
       " 'near_name_jaro_mean': [0.5990131, 0.14566767],\n",
       " 'near_name_jaro_max': [0.9246227, 0.112311274],\n",
       " 'name_jaro_mean_rate': [1.17415, 0.75928885],\n",
       " 'name_jaro_max_rate': [0.73229, 0.27749673],\n",
       " 'near_name_jaro_mean_rate': [1.1688758, 0.76395464],\n",
       " 'near_name_jaro_max_rate': [0.73066276, 0.2782531],\n",
       " 'categories_gesh_mean': [0.45021054, 0.16041645],\n",
       " 'categories_gesh_max': [0.8934652, 0.18464169],\n",
       " 'near_categories_gesh_mean': [0.45295003, 0.1621628],\n",
       " 'near_categories_gesh_max': [0.89459765, 0.18372972],\n",
       " 'categories_gesh_mean_rate': [1.3444637, 0.71897376],\n",
       " 'categories_gesh_max_rate': [0.65741974, 0.31307998],\n",
       " 'near_categories_gesh_mean_rate': [1.3444862, 0.7269207],\n",
       " 'near_categories_gesh_max_rate': [0.65774196, 0.31624898],\n",
       " 'categories_leven_mean': [13.444308, 6.6503487],\n",
       " 'categories_leven_min': [2.9128094, 5.704073],\n",
       " 'near_categories_leven_mean': [13.410719, 6.695578],\n",
       " 'near_categories_leven_min': [2.9045722, 5.7120137],\n",
       " 'categories_leven_mean_rate': [0.7467077, 0.7704559],\n",
       " 'categories_leven_min_rate': [2.024397, 1.5368462],\n",
       " 'near_categories_leven_mean_rate': [0.75811625, 0.9181195],\n",
       " 'near_categories_leven_min_rate': [2.0135462, 1.5323894],\n",
       " 'categories_jaro_mean': [0.6192549, 0.1293145],\n",
       " 'categories_jaro_max': [0.94835114, 0.116735004],\n",
       " 'near_categories_jaro_mean': [0.6217563, 0.13072947],\n",
       " 'near_categories_jaro_max': [0.94931644, 0.11579985],\n",
       " 'categories_jaro_mean_rate': [1.1727746, 0.40448013],\n",
       " 'categories_jaro_max_rate': [0.7595728, 0.24773462],\n",
       " 'near_categories_jaro_mean_rate': [1.1711832, 0.4081076],\n",
       " 'near_categories_jaro_max_rate': [0.75936586, 0.24919231]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_std_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:00, 93.69it/s] \n"
     ]
    }
   ],
   "source": [
    "for n,c in tqdm(enumerate(num_cols)):\n",
    "    num_features_fold0[num_features_fold0[:,n] == np.inf,n] = np.nan\n",
    "    num_features_fold0[num_features_fold0[:,n] == -np.inf,n] = np.nan\n",
    "    num_features_fold0[:,n] = (num_features_fold0[:,n] - mean_std_dict[c][0]) / (mean_std_dict[c][1]) \n",
    "    \n",
    "    num_features_fold1[num_features_fold1[:,n] == np.inf,n] = np.nan\n",
    "    num_features_fold1[num_features_fold1[:,n] == -np.inf,n] = np.nan\n",
    "    num_features_fold1[:,n] = (num_features_fold1[:,n] - mean_std_dict[c][0]) / (mean_std_dict[c][1]) \n",
    "num_features_fold0 = np.nan_to_num(num_features_fold0)\n",
    "num_features_fold1 = np.nan_to_num(num_features_fold1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train_fold0,train_fold1]).reset_index(drop=True)\n",
    "num_features_all = np.concatenate([num_features_fold0,num_features_fold1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:0==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "100%|██████████| 66309/66309 [4:10:33<00:00,  4.41it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:1==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66309/66309 [4:10:17<00:00,  4.42it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:2==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66309/66309 [4:09:26<00:00,  4.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:3==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66309/66309 [4:09:33<00:00,  4.43it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============start epoch:4==============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66309/66309 [4:09:43<00:00,  4.43it/s]  \n",
      "2022-06-27 21:19:26,510 - INFO - [roberta] done in 75027 s\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# train\n",
    "# ================================\n",
    "with timer(\"roberta\"):\n",
    "    set_seed(SEED)\n",
    "    train_text,train_near_text, train_num_features, train_labels = \\\n",
    "    train_all[\"text\"].values, train_all[\"near_text\"].values,num_features_all, train_all[\"target\"].values\n",
    "        \n",
    "    train_ = FourSquareDataset(train_text,train_near_text,train_num_features,\n",
    "                               tokenizer,max_len,train_labels)\n",
    "        \n",
    "    # loader\n",
    "    train_loader = DataLoader(dataset=train_, batch_size=BATCH_SIZE, shuffle = True ,pin_memory=True,num_workers=8)\n",
    "        \n",
    "    # model\n",
    "    model = bert_model()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optimizer, scheduler\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=lr,\n",
    "                      betas=beta,\n",
    "                      weight_decay=weight_decay,\n",
    "                      )\n",
    "    num_train_optimization_steps = int(len(train_loader) * n_epochs)\n",
    "    num_warmup_steps = int(num_train_optimization_steps * num_warmup_steps_rate)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=num_warmup_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    best_score = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f\"============start epoch:{epoch}==============\")\n",
    "        model.train() \n",
    "        scaler = GradScaler()\n",
    "        train_losses_batch = []\n",
    "        for i, d in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
    "            d = collate(d)\n",
    "            ids = d[\"input_ids\"].to(device)\n",
    "            mask = d['attention_mask'].to(device)\n",
    "            token_type_ids = d[\"token_type_ids\"].to(device)\n",
    "            labels = d['label'].to(device)\n",
    "            num_features = d[\"num_feature\"].to(device)\n",
    "            labels = labels.unsqueeze(-1)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                output = model(ids,mask,token_type_ids,num_features)\n",
    "                loss = criterion(output, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "        torch.save(model.state_dict(), MODEL_PATH_BASE + f\"_{epoch}.pth\") # Saving current best model\n",
    "        if epoch == 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
